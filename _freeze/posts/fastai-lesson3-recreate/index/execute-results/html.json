{
  "hash": "b3f2c7e20cf151bacd10993c9649f0d0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: FastAI Lesson 3 \"Further Reseach\"\n---\n\nHi! This is my attempt to do the \"Futher Research\" part of the [FastAI Fastbook Chapter 4](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb) which I recreate using full MNIST datasets and heavily inspired by [this blog](https://medium.com/@jackdriscoll_90855/fast-ai-chapter-4-full-mnist-challenge-8558b1e0564a)\n\nThis is intended to be a learning exercise to slowly learn PyTorch and the FastAI library.\n\n::: {#22a352c2 .cell execution_count=1}\n``` {.python .cell-code}\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\n```\n:::\n\n\n## Part 1: Pixel Similarity\n\nIn this recreation, we are using full [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) consisting from 0-9 handwriting letter\n\nWe first \"download\" the full MNIST dataset and list all the train and validation path\n\n::: {#fa2b0ced .cell execution_count=2}\n``` {.python .cell-code}\nmnist_path = untar_data(URLs.MNIST)\ntrain_path = (mnist_path/'training').ls()\nvalid_path = (mnist_path/'testing').ls()\n```\n:::\n\n\nWe then create a dictionary where the value is the tensor of each image and we create the stacked tensor dictionary where the value is stacked tensor of all images for each digits\n\n::: {#a36bacf6 .cell execution_count=3}\n``` {.python .cell-code}\ntensors_train = {}\nfor digit in train_path:\n    tensors_train[digit.name] = [tensor(Image.open(picture)) for picture in digit.ls()]\n```\n:::\n\n\n::: {#bd8a0847 .cell execution_count=4}\n``` {.python .cell-code}\nstacked_tensors_train = {}\nfor digit_tensor in tensors_train.keys():\n    stacked_tensors_train[digit_tensor] = torch.stack(tensors_train[digit_tensor]).float()/255\n```\n:::\n\n\nBy selecting which keys and calculate the mean by selecting the first dimension (0), we are able to see the average images of each class.\n\n::: {#34a37110 .cell execution_count=5}\n``` {.python .cell-code}\nmean9 = stacked_tensors_train['9'].mean(0)\nshow_image(mean9)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=93 height=93}\n:::\n:::\n\n\nLet us see the distance between one of the 9 and the average 9\n\n::: {#1368a02a .cell execution_count=6}\n``` {.python .cell-code}\na_9 = stacked_tensors_train['9'][0]\nshow_image(a_9)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=93 height=93}\n:::\n:::\n\n\n::: {#4da11eec .cell execution_count=7}\n``` {.python .cell-code}\ndist_9_abs = (a_9 - mean9).abs().mean()\ndist_9_sqr = ((a_9 - mean9)**2).mean().sqrt()\ndist_9_abs, dist_9_sqr\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\n(tensor(0.1066), tensor(0.2059))\n```\n:::\n:::\n\n\n::: {#b8ed432a .cell execution_count=8}\n``` {.python .cell-code}\nmean0 = stacked_tensors_train['0'].mean(0)\n\ndist_0_abs = (a_9 - mean0).abs().mean()\ndist_0_sqr = ((a_9 - mean0)**2).mean().sqrt()\ndist_0_abs, dist_0_sqr\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\n(tensor(0.1844), tensor(0.3134))\n```\n:::\n:::\n\n\nFrom the calculation we can see that the distance between 9 and the \"mean\" 9 is less than the distance between 9 and the \"mean\" 0. By using this distance difference we can classify the digits.\n\nThis can be calculated using pytorch F class that have similar result\n\n::: {#ce813523 .cell execution_count=9}\n``` {.python .cell-code}\nprint(F.l1_loss(a_9, mean9), F.mse_loss(a_9, mean9).sqrt())\nprint(F.l1_loss(a_9, mean0), F.mse_loss(a_9, mean0).sqrt())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(0.1066) tensor(0.2059)\ntensor(0.1844) tensor(0.3134)\n```\n:::\n:::\n\n\n::: {#0a2b500f .cell execution_count=10}\n``` {.python .cell-code}\ntensors_valid = {}\nfor digit in valid_path:\n    tensors_valid[digit.name] = [tensor(Image.open(picture)) for picture in digit.ls()]\n\nstacked_tensors_valid = {}\nfor digit_tensor in tensors_valid.keys():\n    stacked_tensors_valid[digit_tensor] = torch.stack(tensors_valid[digit_tensor]).float()/255\n```\n:::\n\n\nNow instead calculate the mean for each digits\n\n::: {#29b5e5e2 .cell execution_count=11}\n``` {.python .cell-code}\nmodel_mean = {}\nfor digit_tensor in stacked_tensors_valid.keys():\n    model_mean[digit_tensor] = stacked_tensors_train[digit_tensor].mean(0)\n\nfor digit_mean in model_mean.keys():\n    show_image(model_mean[digit_mean])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-3.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-4.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-5.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-6.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-7.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-8.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-9.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-10.png){width=93 height=93}\n:::\n:::\n\n\n::: {#9356a7ce .cell execution_count=12}\n``` {.python .cell-code}\ndef mnist_distance(test_image,mean_image): \n    return (test_image-mean_image).abs().mean((-1,-2))\n```\n:::\n\n\n::: {#94330035 .cell execution_count=13}\n``` {.python .cell-code}\nprint(stacked_tensors_valid['0'].shape, model_mean['0'].shape)\nvalid_dist = mnist_distance(stacked_tensors_valid['0'], model_mean['0'])\nvalid_dist.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([980, 28, 28]) torch.Size([28, 28])\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\ntorch.Size([980])\n```\n:::\n:::\n\n\nmnist_distance will calculate the distance each stack of stack_tensors_valid['0'] with model mean of '0' with resulted a torch size equal to the number of stack of stack_tensor_valid\n\n::: {#2c7a842d .cell execution_count=14}\n``` {.python .cell-code}\nvalid_dist = torch.stack([mnist_distance(stacked_tensors_valid['0'], model_mean[o]) for o in model_mean.keys()])\nvalid_dist, valid_dist.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```\n(tensor([[0.1298, 0.1337, 0.1540,  ..., 0.1307, 0.1248, 0.1627],\n         [0.1822, 0.1556, 0.1623,  ..., 0.2321, 0.2238, 0.2341],\n         [0.1781, 0.1580, 0.1730,  ..., 0.2318, 0.2206, 0.2314],\n         ...,\n         [0.1946, 0.1567, 0.1768,  ..., 0.2283, 0.2128, 0.2416],\n         [0.1775, 0.1466, 0.1838,  ..., 0.2144, 0.1954, 0.2101],\n         [0.1976, 0.1463, 0.1867,  ..., 0.2235, 0.2095, 0.2265]]),\n torch.Size([10, 980]))\n```\n:::\n:::\n\n\nFrom there we improve so that it calculate the distance to each digit of model mean and stack it.\n\n::: {#8fbaed28 .cell execution_count=15}\n``` {.python .cell-code}\nmin_values, min_indices = torch.min(valid_dist, dim=0)\nis_correct = min_indices == int(list(model_mean.keys())[0])\naccuracy_each = is_correct.float().mean()\naccuracy_each\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```\ntensor(0.8153)\n```\n:::\n:::\n\n\nWe can then calculate the accuracy by:\n\n1. On valid_dist stack_tensor we find the index of minimum value\n2. We then check if the minimum indices is the same as the correct digit\n3. calculate the accuracy by calculating the mean of the is_correct bollean list\n\n::: {#24fa71a4 .cell execution_count=16}\n``` {.python .cell-code}\ndef calc_acc(an_image_tensor, keys):\n    valid_dist = torch.stack([mnist_distance(an_image_tensor, model_mean[o]) for o in model_mean.keys()])\n    min_values, min_indices = torch.min(valid_dist, dim=0)\n    is_correct = min_indices == int(list(model_mean.keys())[int(keys)])\n    accuracy = is_correct.float().mean()\n    return accuracy\n\nacc = torch.stack([calc_acc(stacked_tensors_valid[keys], keys) for keys in stacked_tensors_valid.keys()])\nacc.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\ntensor(0.6610)\n```\n:::\n:::\n\n\nCombine everything in calc_acc, and instead of only calculating the accuracy for one digit, we calculate for every digit and find the mean. The mean will be our model's accuracy.\n\n## Part 2: Learner\n\nNow we want to improve the model accuracy by using regression model\n\nFor the data preparation, instead of using dictionary, we create a list of pair where the first dimension is x_data and second dimension is y_data\n\n::: {#267d7fd9 .cell execution_count=17}\n``` {.python .cell-code}\ntrain_path = (mnist_path/'training').ls()\nvalid_path = (mnist_path/'testing').ls()\n```\n:::\n\n\n::: {#b2fbe5bf .cell execution_count=18}\n``` {.python .cell-code}\ntensors_train = [(tensor(Image.open(each_image)), int(digit_file.name)) for digit_file in train_path for each_image in digit_file.ls()]\ntensors_valid = [(tensor(Image.open(each_image)), int(digit_file.name)) for digit_file in valid_path for each_image in digit_file.ls()]\nnp.random.shuffle(tensors_train)\nnp.random.shuffle(tensors_valid)\n```\n:::\n\n\nSo what tensors_train contain is:\n\n- tensors is list with 60.000 element\n- tensors[] is tuple with 2 elements\n- tensors[][0] is tensor of images value 28x28\n- tensors[][1] is the label\n\n::: {#79651428 .cell execution_count=19}\n``` {.python .cell-code}\nprint(len(tensors_train))\nprint(len(tensors_train[0]))\nprint(len(tensors_train[0][0]))\nprint(tensors_train[0][1])\nprint(len(tensors_valid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n60000\n2\n28\n5\n10000\n```\n:::\n:::\n\n\n::: {#6668776f .cell execution_count=20}\n``` {.python .cell-code}\ntrain_x = torch.cat([tensors_train[each_stack][0].view(-1, 784).float()/255 for each_stack in range(len(tensors_train))])\ntrain_y = tensor([tensors_train[each_stack][1] for each_stack in range(len(tensors_train))])\n\nvalid_x = torch.cat([tensors_valid[each_stack][0].view(-1, 784).float()/255 for each_stack in range(len(tensors_valid))])\nvalid_y = tensor([tensors_valid[each_stack][1] for each_stack in range(len(tensors_valid))])\n```\n:::\n\n\ntrain_ and valid_ structure:\n\n- train_ is tensor with 48.000 stack of 784 tensor of flattened image (28x28)\n- valid_ is tensor with len 48.000 which are the class label of each train_ stack\n\n::: {#a8a857b4 .cell execution_count=21}\n``` {.python .cell-code}\nprint(train_x.shape)\nprint(train_y.shape)\nprint(valid_x.shape)\nprint(valid_y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([60000, 784])\ntorch.Size([60000])\ntorch.Size([10000, 784])\ntorch.Size([10000])\n```\n:::\n:::\n\n\n::: {#79ab46a7 .cell execution_count=22}\n``` {.python .cell-code}\ndset_train = list(zip(train_x, train_y))\ndset_valid = list(zip(valid_x, valid_y))\n```\n:::\n\n\ndset and valid_dset is pair of data and label:\n\n- dset is length of 48.000 data\n- dset[] is each pair\n- dset[][0] is the x and dset[][1] is the y\n\n::: {#610edc91 .cell execution_count=23}\n``` {.python .cell-code}\nprint(len(dset_train))\nprint(len(dset_train[0]))\nprint(len(dset_train[0][0]))\nprint(dset_train[0][0].shape)\nprint(dset_train[0][1].shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n60000\n2\n784\ntorch.Size([784])\ntorch.Size([])\n```\n:::\n:::\n\n\n::: {#0d7772c8 .cell execution_count=24}\n``` {.python .cell-code}\nprint(len(dset_valid))\nprint(len(dset_valid[0]))\nprint(len(dset_valid[0][0]))\nprint(dset_valid[0][1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10000\n2\n784\ntensor(7)\n```\n:::\n:::\n\n\nCreate function to initialize parameter that give output a tensor with desired size\n\n::: {#6799218e .cell execution_count=25}\n``` {.python .cell-code}\ndef init_params(size, std=1.0): return (torch.randn(size) * std).requires_grad_()\n```\n:::\n\n\n::: {#eeb8b1cf .cell execution_count=26}\n``` {.python .cell-code}\nweights = init_params(((28*28), 10))\nbias = init_params((1, 10))\n```\n:::\n\n\nBecause this one is a linear type classifier, we need to have 10 stack of weights for each digits. It is different from neural networks based where we used 1 set of weights\n\n::: {#0c0e77dd .cell execution_count=27}\n``` {.python .cell-code}\nprint(train_x[0].shape, \"-> its equal to [1, 784]\")\nprint(weights.shape)\npred = train_x[0]@weights + bias\nprint(pred)\nprint(pred.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([784]) -> its equal to [1, 784]\ntorch.Size([784, 10])\ntensor([[-12.8129,  15.3278,   4.1641,   1.6066,   4.2265,  11.4496,  19.4137,   2.0250,  25.2653,  -6.6110]], grad_fn=<AddBackward0>)\ntorch.Size([1, 10])\n```\n:::\n:::\n\n\nThis doenst quiet represent anything yet. We need to use softmax to convert the output to probabilities.\n\n::: {#858536c0 .cell execution_count=28}\n``` {.python .cell-code}\ndef linear1(x_matrix): return x_matrix@weights + bias\npred = linear1(train_x)\npred_prob = F.softmax(pred, dim=1)\nprint(pred_prob)\nprint(pred_prob.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[2.8946e-17, 4.8186e-05, 6.8328e-10,  ..., 8.0465e-11, 9.9708e-01, 1.4290e-14],\n        [1.5549e-08, 1.6647e-08, 4.7936e-06,  ..., 2.1437e-15, 1.4793e-04, 1.3994e-09],\n        [8.0650e-07, 3.7267e-06, 1.1988e-05,  ..., 1.3478e-02, 5.2920e-04, 3.1845e-03],\n        ...,\n        [6.0731e-07, 4.7170e-01, 2.7054e-08,  ..., 2.0115e-10, 4.8844e-01, 3.8189e-02],\n        [9.2131e-01, 1.5331e-07, 3.5232e-07,  ..., 2.0329e-11, 1.8150e-04, 1.7378e-14],\n        [6.7177e-07, 3.1652e-04, 1.0417e-06,  ..., 4.6246e-02, 3.6580e-07, 4.9520e-09]], grad_fn=<SoftmaxBackward0>)\ntorch.Size([60000, 10])\n```\n:::\n:::\n\n\npred_prob store probability of each image for all digits.\n\nThen we try to calculate the accuracy\n\n::: {#e2da9199 .cell execution_count=29}\n``` {.python .cell-code}\nacc = tensor([torch.argmax(pred_prob[i]) == train_y[i] for i in range(len(pred_prob))])\nacc.float().mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\ntensor(0.0683)\n```\n:::\n:::\n\n\nWhat it acc did:\n\n- For every stack of pred_prob tensor, it find the index of the highest probability and return the index.\n- After finding the index it do bollean check to see if the index is equal to the actual label.\n- We then calculate the mean to see the accuracy\n\nLets try to update the weight to see if there is any significant change\n\n::: {#da99e5b9 .cell execution_count=30}\n``` {.python .cell-code}\nweights[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\ntensor([ 1.1152,  0.6064,  0.9859,  1.4940,  1.9783,  0.3804,  0.0418,  0.2844, -0.3880, -0.7500], grad_fn=<SelectBackward0>)\n```\n:::\n:::\n\n\n::: {#c38f6d44 .cell execution_count=31}\n``` {.python .cell-code}\nwith torch.no_grad(): weights[0] *= 1.0001\npred = linear1(train_x)\npred_prob = F.softmax(pred, dim=1)\nacc = tensor([torch.argmax(pred_prob[i]) == train_y[i] for i in range(len(pred_prob))])\nacc.float().mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=74}\n```\ntensor(0.0683)\n```\n:::\n:::\n\n\nAs you can see the accuracy didnt change visibly. This means that we need to find better metrics to evaluate our model.\n\nBecause the change in weight SHOULD BE change how our model works (for better or worse)\n\nWe will use what we call a binary cross entory loss (I will make other post on that later, for now please look for your own reference)\n\n::: {#b64fde5f .cell execution_count=32}\n``` {.python .cell-code}\ndef mnist_loss(predictions, targets):\n    loss = F.cross_entropy(predictions, targets.squeeze())\n    return loss\n```\n:::\n\n\n::: {#e8497ee3 .cell execution_count=33}\n``` {.python .cell-code}\nloss = mnist_loss(pred, train_y)\nprint(loss)\n\nloss.backward() \nweights.grad.shape,weights.grad.mean(),bias.grad.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(14.6460, grad_fn=<NllLossBackward0>)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=76}\n```\n(torch.Size([784, 10]), tensor(-1.5205e-10), torch.Size([1, 10]))\n```\n:::\n:::\n\n\nHere we can have gradient for each weight of each pixel (therefore we have torch.Size([784, 10])\n\n::: {#f8286512 .cell execution_count=34}\n``` {.python .cell-code}\ndef calc_grad(x_data, y_data, model):\n    preds = model(x_data)\n    loss = mnist_loss(preds, y_data)\n    loss.backward()\n\ncalc_grad(train_x, train_y, linear1)\nweights.grad.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\ntensor(-3.0411e-10)\n```\n:::\n:::\n\n\n::: {#39b8aa92 .cell execution_count=35}\n``` {.python .cell-code}\nprint(train_x.shape)\nprint(train_y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([60000, 784])\ntorch.Size([60000])\n```\n:::\n:::\n\n\nFor every epoch, we need to set the gradient into zero to make sure it didnt add onto existing gradient (as it do in in default)\n\n::: {#e001010e .cell execution_count=36}\n``` {.python .cell-code}\ntrain_batches = DataLoader(dset_train, batch_size=256)\nvalid_batches = DataLoader(dset_valid, batch_size=256)\nxb,yb = first(train_batches)\n```\n:::\n\n\n::: {#58159cb4 .cell execution_count=37}\n``` {.python .cell-code}\nprint(len(train_batches))\nprint(xb.shape)\nprint(yb.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n235\ntorch.Size([256, 784])\ntorch.Size([256])\n```\n:::\n:::\n\n\nSo what dataloader do is to create a batch of x and y data from the dset_ dataset (thats why it has 60.000 / 256 = 235 len)\n\n::: {#8f82cdd3 .cell execution_count=38}\n``` {.python .cell-code}\ndef train_epoch(model, learn_rate, params):\n    for x_data, y_data in train_batches:\n        calc_grad(x_data, y_data, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n```\n:::\n\n\nWhat it did is for each epoch it learn update the params (in this case is weights AND bias thats why its done in loop) based on the gradients that calculated in each loop train dataset\n\n::: {#d28a6ef3 .cell execution_count=39}\n``` {.python .cell-code}\ndef batch_accuracy(x_data, y_data):\n    preds = F.softmax(x_data, dim=1)\n    correct = torch.argmax(preds, dim=1) == y_data\n    return correct.float().mean()\n```\n:::\n\n\nWe still need to use accuracy to measure the final performance after adjustment\n\n::: {#5392b15e .cell execution_count=40}\n``` {.python .cell-code}\nbatch_accuracy(linear1(train_x), train_y)\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\ntensor(0.0683)\n```\n:::\n:::\n\n\n::: {#f01d0c02 .cell execution_count=41}\n``` {.python .cell-code}\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_batches]\n    return round(torch.stack(accs).mean().item(), 4)\n\nvalidate_epoch(linear1)\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\n0.0753\n```\n:::\n:::\n\n\nThe accuracy is used to validate each epoch\n\n::: {#9d222b97 .cell execution_count=42}\n``` {.python .cell-code}\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n```\n\n::: {.cell-output .cell-output-display execution_count=85}\n```\n0.8472\n```\n:::\n:::\n\n\nFrom just one epoch we can get the 84% accuracy\n\nNow lets try to train 20 epoch\n\n::: {#14e67187 .cell execution_count=43}\n``` {.python .cell-code}\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8754 0.8839 0.8918 0.8982 0.9018 0.9041 0.9056 0.9065 0.9085 0.9093 0.9112 0.9131 0.9138 0.9144 0.9141 0.9137 0.9142 0.9142 0.9144 0.9145 \n```\n:::\n:::\n\n\nHere we go, just using very2 basic linear regression model we can have 91% accuracy.\n\nThe next iteration of model using neural network will be done on another post.\n\nPlease kindly submit your comments and suggestions to my email address: fahmi.rizaldi@gmail.com\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}